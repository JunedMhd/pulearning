%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Augmented Logistic Regression for Training on Incomplete Positive and Unlabeled Datasets}

\begin{document} 

\twocolumn[
\icmltitle{Augmented Logistic Regression for Training on Incomplete Positive and Unlabeled Datasets}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Joseph Perla}{jjperla@cs.ucsd.edu}
\icmladdress{University of California, San Diego,
            9500 Gilman Dr, La Jolla, California 92093 USA}
\icmlauthor{Charles Elkan}{elkan@ucsd.edu}
\icmladdress{University of California, San Diego,
            9500 Gilman Dr, La Jolla, California 92093 USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{logistic regression, multinomial logistic regression, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
In a common supervised learning task, we are given a training set where all samples are labeled either positive or negative. Semi-supervised algorithms operate on input training sets with some positive and negative labels, and some unlabeled data.  We studied the positive-only problem where the training set has an incomplete set of positive labels, no negative labels, and the remainder unlabeled.  In this paper, we assume that the labeled positive samples are selected uniformly at random from the set of all positive samples in the training set with probability $c$. Therefore, the labeled and unlabeled positive samples in the training set have identical distributions.  We develop two simple modifications of logistic regression which classify the true labels of the training set given only the restricted set of labels.

\end{abstract} 


\section{Introduction}

In a common supervised learning task, we are given a training set where all samples are labeled either positive or negative. Semi-supervised algorithms operate on input training sets with some positive and negative labels, and some unlabeled data.  We studied the positive-only problem where the training set has an incomplete set of positive labels, no negative labels, and the remainder unlabeled.  In this paper, we assume that the labeled positive samples are selected uniformly at random from the set of all positive samples in the training set with probability $c$. Therefore, the labeled and unlabeled positive samples in the training set have identical distributions.

\section{Algorithms}

\subsection{Optimal}
Learning on a training set with only some positively labeled data cannot achieve a higher accuracy on the test data than training on the same training dataset where all labels are known (if so, then one could throw out the negative labels in the latter case and achieve the same results).  Therefore, we compare our results to training an SGD logistic regression classifier (LR) and a standard RBF-kernel SVM (SVM) on the training set with all labels known and correctly labeled.  This SVM and all SVMs in this paper were trained using a linear kernel, because the number of features exceeds the number of examples, and three-fold cross-validation was used to determine which value of the regularizer $C \in {1e-5, ... 1e5}$ performs optimally on the training set.

\subsection{Baseline}

As a baseline, we show results for running standard logistic regression and an SVM using the positive labeled data versus the unlabeled data.  This overfits to the subset of labeled positive examples which misclassifies many positive examples on the test set.


\subsection{Previous Work}
Previous work\cite{elkan08} achieved maximal classification accuracy with a Biased SVM, an SVM where the positive labeled examples were given a different misclassification penalty than the unlabeled examples.  The standard SVM algorithm multiplies all misclassification penalties by the same constant factor $C$ for all examples. A Biased SVM has two values $C_u$ and $C_p$ which multiply the penalty by different factors for errors on unlabeled and positive examples, respectively.

\subsection{Weighted SVM}
We also show results for a weighted SVM algorithm, where the SVM classifier is run once to assign probabilities of assignment, then those probabilities are used to weigh the inputs to a second SVM classifier which produces the final output.  The results of the Weighted SVM are strong in the previous work, but the results here are weaker  due to poor parameter selection for the SVMs.

\subsection{Modified Logistic Regression}
The contribution of this paper is a modified version of logistic regression that we solve using SGD (Modified LR).  Whereas standard logistic regression optimizes the following objective:

$$ \frac{1}{1 + e^{\overline{w} \cdot \overline{x}}} $$

Our modified logistic regression adds a term $b^2$ which allows a proportion of the unlabeled examples to be classified as positive. From the final value of $b^2$, we calculate the proportion of positive examples which are labeled (taking the assumption that the labeled positive examples are selected completely at random and have no bias in which ones are labeled).

$$ \frac{1}{1 + b^2 + e^{\overline{w} \cdot \overline{x}}} $$

This algorithm which, in a principled way, takes into account the unlabeled positive data, performs nearly as well as the classifiers run using the fully labeled training set, and much better than previous work and the baseline naive classifiers.


\section{Estimating the proportion of labeled examples}
Let $s$ be a binary indicator variable signifying if an example is labeled, and let $y$ be the label of the example with features $x$. Let $c = p(s=1|y=1)$ the proportion of truly positive examples which are visibly labeled as such. $P$ is the positively labeled data, and $U$ is the unlabeled data. From $c$, we can easily derive $p(y=1)$ the fraction of the data which are truly positive via Bayes' Rule:

$$ p(y=1) = \frac{p(s=1)}{p(s = 1|y = 1)} =  \frac{|P|}{p(s = 1|y = 1)|P+U|} = \frac{|P|}{c|P+U|} $$

Previous work on this topic\cite{elkan08} contains a math error.  Significantly, it concludes that $g(x) = p(s=1|x) = c = p(s=1|y=1)$. This is incorrect, because it assumes that if a given $x \in P$, then $p(y=1|x) = 1$, however a given set of features may probabilistically be positive for some examples and negative for others. This occurs often when the positive and negative distributions overlap (and never when they are separable). The correct derivation is therefore

$$ g(x) = p(s=1|x) $$
$$ g(x) = p(s=1|x,y=1)p(y=1|x) + p(s=1|x,y=0)p(y=0|x)$$
$$ g(x) = p(s=1|x,y=1)p(y=1|x) + p(s=1|y=0)p(y=0|x)$$
$$ g(x) = p(s=1|x,y=1)p(y=1|x) + 0 \cdot p(y=0|x)$$
$$ g(x) = p(s=1|x,y=1)p(y=1|x)$$
$$ g(x) = c \cdot p(y=1|x)$$

We can derive various estimators of $c$ using the classifiers, and the best seems to be based on $b$ calculated with the Modified Logistic Regression algorithm above.

$$c \approx \frac{1}{1 + b^2}$$

This is because Modified Logistic Regression generates calibrated probabilities in the same way that standard logistic regression does, but instead of having a maximum probability of $1$ for a positive label, it can only have a maximum probability of $p(s=1|y=1)=c$, since any positive example has at most that probability of being labeled.  Modified Logistic Regression calibrates its probabilities directly using the data and finds $b$ during training of

$$g(x) = \frac{1}{1 + b^2 + e^{\overline{w} \cdot \overline{x}}}$$

which has a maximum when $\overline{w} \cdot \overline{x}=-\infty$

$$g(x) = \frac{1}{1 + b^2} \approx c$$


\section{The Multinomial Regression Model}

We model the positive-only problem with a multinomial regression in the following way: assume there are 3 classes.  One class is positive and labeled ($P_L$), one is positive and unlabeled ($P_U$), and the last is negative and unlabeled (N).  There are no negative labeled samples.  We use the standard multinomial regression probabilities, but we share the weights $w_p$ between the positive-labeled ($P_L$) and positive-unlabeled ($P_U$) classes. Finally, we include a bias term $b$ which biases the constant fraction of the positive samples which are labeled.  $Z_N$ is the normalization factor:

$$
p(k=P_L | x, w_p, w_n) =  \frac{e^{w_p \cdot x}}{Z_N} = \frac{e^{w_p \cdot x}}{e^{w_p \cdot x} + \cdot e^{w_p \cdot x + b} + e^{w_n \cdot x}} = \frac{e^{w_p \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}}
$$

$$
p(k=P_U | x, w_p, w_n) =  \frac{e^{w_p \cdot x + b}}{Z_N} = \frac{e^{w_p \cdot x + b}}{e^{w_p \cdot x} + e^{w_p \cdot x + b} + e^{w_n \cdot x}} = \frac{e^{w_p \cdot x + b}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}}
$$

$$
p(k=N | x, w_p, w_n) =  \frac{e^{w_n \cdot x}}{Z_N} = \frac{e^{w_n \cdot x}}{e^{w_p \cdot x} + e^{w_p \cdot x + b} + e^{w_n \cdot x}} = \frac{e^{w_n \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}}
$$

Notice that, $\forall x$,

$$
\frac{p(k=P_L | x, w_p, w_n)}{p(k=P_L | x, w_p, w_n) + p(k=P_U | x, w_p, w_n)} = \frac{1}{1 + e^b} = c.
$$

Thus, $c$ is the constant fraction of positive samples which are labeled.

Finally, we can divide by $e^{w_p \cdot x}$ through all the terms in order to consolidate parameters. Let $w = w_n - w_p$,

$$
p(k=P_L | x, w_p, w_n) = \frac{e^{w_p \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} = \frac{1}{1 + e^b + e^{w \cdot x}}
$$

$$
p(k=P_U | x, w_p, w_n) = \frac{e^{w_p \cdot x + b}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} = \frac{e^b}{1 + e^b + e^{w \cdot x}}
$$

$$
p(k=N | x, w_p, w_n) = \frac{e^{w_n \cdot x}}{(1 + e^{b}) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} = \frac{e^{w \cdot x}}{1 + e^b + e^{w \cdot x}}
$$

So we only need to learn the vector $w$.

\section{Learning $\beta = b, w$ from $D$}

Assuming we have independently drawn data $D$, the conditional log likelihood, where $\beta = c, w_p, w_n$ is
$$
\log{p(D | \beta)} = \log \prod_{j=1...n}{p(\ell_j, x_j | \beta)} = \sum_{j=1...n}{\log{p(\ell_j, x_j | \beta)}} = \sum_{j=1...n}{\log{p(\ell_j | x_j, \beta)}} \cdot p(x_j | \beta)
$$

where $\ell_j$ is an indicator variable describing whether the sample is labeled or not. $\ell_j = 1$ if and only if $k_j=P_L$, otherwise $k_j = N$ or $k_j=P_U$ and $\ell_j = 0$.  Also note that $p(x_j|\beta) = p(x_j)$ and we assume uninformative uniform prior for $x_j$, so let $p(x_j|\beta) = 1$.

The maximum likelihood (ML) estimate $\hat \beta$ is the value of $\beta$ maximizing the likelihood of the data $D$:
$$
\hat \beta = arg max_{\beta} p(D | \beta) = arg max_{\beta} \log{p(D | \beta)}.
$$

\subsection{Stochastic Gradient}
We want to use stochastic gradient following to learn the parameters $\beta$. So take the partial derivatives of the log conditional likelihood (LCL) for each parameter.

\subsubsection{$ \nabla_{w_i}{Err_R (D, \beta)}$}

$$
\nabla_{w_i}{Err_R (D, \beta)} = \frac{\partial}{\partial w_i} \log{p(D|\beta)} =  \sum_{j=1...n}{\frac{\partial}{\partial w_i}\log{p(\ell_j | x_j, \beta)}}
$$

so for each $j$,

$$
 = \frac{\partial}{\partial w_i}\log{p(\ell_j | x_j, \beta)}
$$

$$
 = \frac{\partial}{\partial w_i}
		\log{\Bigg(
			I(\ell_j=1) \cdot \left( \frac{1}{1 + e^b + e^{w \cdot x}} \right) + 
			I(\ell_j=0) \cdot \left(\frac{e^b}{1 + e^b + e^{w \cdot x}} + \frac{e^{w \cdot x}}{1 + e^b + e^{w \cdot x}} \right)
		\Bigg)}
$$

$$
 = \frac{\partial}{\partial w_i}
		\log{\Bigg(
		    \frac{
			I(\ell_j=1) + 
			I(\ell_j=0) \cdot \left( e^b + e^{w \cdot x} \right)
		     }{1 + e^b + e^{w \cdot x}}
		\Bigg)}
$$

$$
 = \frac{\partial}{\partial w_i}
		\log{\bigg(
			I(\ell_j=1) + 
			I(\ell_j=0) \cdot \left( e^b + e^{w \cdot x} \right)
		\bigg)}
		- \frac{\partial}{\partial w_i}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$


$$
 = I(\ell_j=1) \cdot \frac{\partial}{\partial w_i}
		\log{(1)} +
    I(\ell_j=0) \cdot \frac{\partial}{\partial w_i}
		\log{ \left( e^b + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial w_i}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$

$$
 = I(\ell_j=0) \cdot \frac{\partial}{\partial w_i}
		\log{  \left( e^b + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial w_i}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$


$$
 = I(\ell_j=0) \cdot 
 		\frac{
			e^{w \cdot x}
		}{
			e^b + e^{w \cdot x}
		} \cdot x_{j,i}
		- \frac{\partial}{\partial w_i}
		 \log{\Big(
			1 + e^b + e^{w \cdot x}
		\Big)}
$$

$$
 = I(\ell_j=0) \cdot 
 		\frac{
			e^{w \cdot x}
		}{
			e^b + e^{w \cdot x}
		} \cdot x_{j,i}
		- \frac{e^{w \cdot x}}{
			1 + e^b + e^{w \cdot x}
		} \cdot x_{j,i}
$$

$$
 = x_{j,i} \cdot
    \Big(	I(\ell_j=0) \cdot p(k_j=N | \ell_j=0, x_j, \beta) - 
		p(k_j=N | x_j, \beta)
    \Big)
$$

\subsubsection{$ \nabla_{b}{Err_R (D, \beta)}$}


Similarly for the parameter $b$ in $\beta$, for each $j$,


$$
 = \frac{\partial}{\partial c}\log{p(\ell_j | x_j, \beta)}
$$


$$
 = I(\ell_j=0) \cdot \frac{\partial}{\partial b}
		\log{  \left( e^b + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial b}
		 \log{\bigg(
			1 + e^b + e^{w \cdot x}
		\bigg)}
$$


$$
 = I(\ell_j=0) \cdot
		\frac{1}{e^b + e^{w \cdot x}}
		-
		 \frac{1}{
			1 + e^b + e^{w \cdot x}
		}
$$


$$
 = I(\ell_j=0) \cdot
		\frac{p(\ell_j=1 | x_j,\beta)}{p(\ell_j=0 | x_j,\beta)}
		-
		 p(\ell_j=1 | x_j,\beta)
$$

\section{Experiments}

\subsection{Synthetic Example}

Here, we reproduce a version of Figure 1 in \cite{elkan08}. 

\label{Synthetic data}
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{syntheticlr.png}
\caption{Blue points are positive examples, red negative. The largest dark blue ellipse is the result of training standard LR on all of the data labeled. The smaller light blue ellpise is the result of training standard LR on the positive vs unlabeled examples. The red ellipse is from training Modified LR on the positive vs unlabeled examples. The features were $x$, $y$, $x \cdot x$, and $y \cdot y$.  Modified LR captures a more representative sample of the positive datapoints, as well as calculating the approximate value of $c = 0.2 \approx 0.196$.   500 positive datapoints were generated using a random gaussian with mean (2, 2) and covariance [[1, 1], [1, 4]].  1000 negative datapoints were generated with mean (-2, -3) and covariance [[4, -1], [-1, 4]]}
\end{figure}


\subsection{SwissProt Dataset}

The algorithms were run on the SwissProt\cite{elkan08} dataset of 7359 proteins which each have a sparse set of 24081 binary features related to characteristics of the proteins. It contains 4558 negative examples, 2453 labeled positive examples, and 348 unlabeled positive examples.   We consider the negative and unlabeled positive examples together as the unlabeled dataset.


\label{ROC for Protein Dataset}
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{rocproteindata.png}
\caption{The ROC curves for various learning algorithms on sets of the protein dataset.  Note that we zoom into the most interesting region in the top left.  As expected, both LR and SVM trained on the true labels perform the best and approximately equally on the test set (10 holdout). Standard LR and SVM on the positive data versus the unlabeled data perform poorly.  The best previous work, Biased SVM, does not perform much better than the baselines.  The Modified Logistic Regression trained on the positive labeled data and unlabeled data performs nearly as well on the test set as the classifiers trained with full knowledge of all positive and negative labels.}
\end{figure}

\label{ROC for Swapped Protein Dataset}
\begin{figure}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{rocswappedproteindata.png}}
\caption{We re-run the algorithms from Figure 1 on the same dataset, but swapping the unlabeled positive examples for the labeled positive examples. Our new dataset contains 4558 negative examples, 2453 unlabeled positive examples, and only 348 labeled positive examples.  Modified LR still performs significantly better than baselines and previous work, although not as well as knowing the full dataset.  These results are impressive because less than 5\% of the examples are labeled, all of which are positive labels. The weighted SVM and other SVMs may not be optimally calibrated.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure} 

\bibliography{icml2014}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
