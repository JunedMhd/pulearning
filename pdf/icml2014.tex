%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% for big figures
\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Modified Logistic Regression for Training on Incomplete Positive and Unlabeled Datasets}

\begin{document} 

\twocolumn[
\icmltitle{Modified Logistic Regression for Training on Incomplete Positive and Unlabeled Datasets}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Joseph Perla}{jjperla@cs.ucsd.edu}
\icmladdress{University of California, San Diego,
            9500 Gilman Dr, La Jolla, California 92093 USA}
\icmlauthor{Charles Elkan}{elkan@ucsd.edu}
\icmladdress{University of California, San Diego,
            9500 Gilman Dr, La Jolla, California 92093 USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{logistic regression, multinomial logistic regression, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
In a common supervised learning task, we are given a training set where all samples are labeled either positive or negative. Semi-supervised algorithms operate on input training sets with some positive and negative labels, and some unlabeled data.  We studied the positive-only problem where the training set has an incomplete set of positive labels, no negative labels, and the remainder unlabeled.  In this paper, we assume that the labeled positive samples are selected uniformly at random from the set of all positive samples in the training set with probability $c$. Therefore, the labeled and unlabeled positive samples in the training set have identical distributions.  We develop two algorithms which are simple modifications to logistic regression: Ceiling Logistic Regression and Positive-Only Logistic Regression. They classify the true labels of the training set given only the restricted set of labels.  In particular, on synthetic data, training Positive-Only Logistic Regression using a small subset of positive labels performs as well as training standard logistic regression using all of the true labels when comparing accuracy on a test set.  We show that we can make strong predictions with very limited positive-only data.

\end{abstract} 

\section{Introduction}

Many datasets do not have full training data that includes a large number of positive and negative labeled data.  With the growth of computers, datasets today very often have a small amount of manually labeled data, and a large amount of unlabeled data. Moreover, it is common for one class to be more expensive or even impossible to label.  Examples problems in the literature include wildlife habitats \cite{ward08}, and molecular biology databases \cite{elkan08}.  Note that this situation is different from most semi-supervised learning because previous work on semi-supervised learning does have some negatively labeled data.  A synonym for \emph{positive-only} data is \emph{presence-only} data \cite{ward08}.

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{syntheticlr.png}}
\caption{Blue points are positive examples, red negative. The largest dark blue ellipse is the result of training standard LR on all of the data labeled. The smaller light blue ellpise is the result of training standard LR on the positive vs unlabeled examples. The red ellipse is from training Ceiling LR on the positive vs unlabeled examples. The features were $x$, $y$, $x \cdot x$, and $y \cdot y$.  Ceiling LR captures a more representative sample of the positive datapoints, as well as calculating the approximate value of $c = 0.2 \approx 0.196$.   500 positive datapoints were generated using a random gaussian with mean (2, 2) and covariance [[1, 1], [1, 4]].  1000 negative datapoints were generated with mean (-2, -3) and covariance [[4, -1], [-1, 4]]}
\label{synthetic}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Algorithms}

\subsection{Optimal}

Learning on a training set with only some positively labeled data cannot achieve a higher accuracy on the test data than training on the same training dataset where all labels are known (if so, then one could throw out the negative labels in the latter case and achieve the same results).  Therefore, we compare our results to training an SGD logistic regression classifier (LR) and a standard RBF-kernel SVM (SVM) on the training set with all labels known and correctly labeled.  This SVM and all SVMs in this paper were trained using a linear kernel and three-fold cross-validation was used to determine which value of the regularizer $C \in \{1e-5, ..., 1e5\}$ performs optimally on the training set.

\subsection{Baseline}

As a baseline, we show results for running standard logistic regression and an SVM using the positive labeled data versus the unlabeled data.  This overfits to the subset of labeled positive examples which misclassifies many positive examples on the test set.

\subsection{Best Published Algorithms}

\subsubsection{Biased SVM}

State of the art in the literature \cite{elkan08} achieves maximal classification accuracy with a Biased SVM, an SVM where the positive labeled examples were given a different misclassification penalty than the unlabeled examples.  The standard SVM algorithm multiplies all misclassification penalties by the same constant factor $C$ for all examples. A Biased SVM has two values $C_u$ and $C_p$ which multiply the penalty by different factors for errors on unlabeled and positive examples, respectively.

\subsubsection{Weighted SVM}

We also show results for a weighted SVM algorithm, where the SVM classifier is run once to assign probabilities of assignment, then those probabilities are used to weigh the inputs to a second SVM classifier which produces the final output.  The results of the Weighted SVM are strong in the literature \cite{elkan08}, but the results here are weaker due to poor parameter selection for the SVMs.

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{rocproteindata.png}}
\caption{The ROC curves for various learning algorithms on sets of the protein dataset.  Note that we zoom into the most interesting region in the top left.  As expected, both LR and SVM trained on the true labels perform the best and approximately equally on the test set (10 holdout). Standard LR and SVM on the positive data versus the unlabeled data perform poorly.  The best previous work, Biased SVM, does not perform much better than the baselines.  The Ceiling Logistic Regression trained on the positive labeled data and unlabeled data performs nearly as well on the test set as the classifiers trained with full knowledge of all positive and negative labels.}
\label{roc}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Ceiling Logistic Regression}

One contribution of this paper is a modified version of logistic regression that we solve using stochastic gradient descent called Ceiling Logistic Regression (CLR).  Whereas standard logistic regression calculates

$$ p(y=1|x) = \frac{1}{1 + e^{w \cdot x}} $$

Our modified logistic regression adds a term $b^2$ which allows a proportion of the unlabeled examples to be classified as positive. From the final value of $b^2$, we calculate the proportion of positive examples which are labeled (taking the assumption that the labeled positive examples are selected uniformly at random and have no bias in which ones are labeled).

$$ p(y=1|x) = \frac{1}{1 + b^2 + e^{w \cdot x}} $$

This algorithm takes into account the unlabeled positive data, performs nearly as well as the classifiers run using the fully labeled training set, and much better than previous work and the baseline naive classifiers.

\subsubsection{Estimating the proportion of labeled examples}

Let $\ell$ be a binary indicator variable signifying if an example is labeled, and let $y$ be the label of the example with features $x$. Let $c = p(\ell=1|y=1)$ the proportion of truly positive examples which are visibly labeled as such. $P$ is the positively labeled data, and $U$ is the unlabeled data. From $c$, we can easily derive $p(y=1)$ the fraction of the data which are truly positive via Bayes's Rule:

\begin{eqnarray*}
p(y=1) &=& \frac{p(s=1)}{p(\ell = 1|y = 1)} \\
 &=&  \frac{|P|}{p(s = 1|y = 1)|P+U|} \\
 &=& \frac{|P|}{c|P+U|}
\end{eqnarray*}

Elkan and Noto \yrcite{elkan08} contains a math error.  Significantly, it concludes that $g(x) = p(\ell=1|x) = c = p(\ell=1|y=1)$. This is incorrect, because it assumes that if a given $x \in P$, then $p(y=1|x) = 1$, however a given set of features may probabilistically be positive for some examples and negative for others. This occurs often when the positive and negative distributions overlap (and never when they are separable). The correct derivation, because $p(\ell=1|y=0)=0$, is

\begin{eqnarray*}
g(x) &=& p(\ell=1|x) \\
 &=& p(\ell=1|x,y=1)p(y=1|x) + 0 \cdot p(y=0|x) \\
 &=& p(\ell=1|x,y=1)p(y=1|x) \\
 &=& c \cdot p(y=1|x)
\end{eqnarray*}

We can derive various estimators of $c$ using the classifiers, and the best seems to be based on $b$ calculated with the Ceiling Logistic Regression algorithm above.

$$c \approx \frac{1}{1 + b^2}$$

This is because Ceiling Logistic Regression generates calibrated probabilities in the same way that standard logistic regression does, but instead of having a maximum probability of $1$ for a positive label, it can only have a maximum probability (ceiling) of $p(\ell=1|y=1)=c$. Any positive example has at most probability $c$ of being labeled.  Ceiling Logistic Regression calibrates its probabilities directly and finds $b$ during training of

$$g(x) = \frac{1}{1 + b^2 + e^{w \cdot x}}$$

which has a maximum when $w \cdot x=-\infty$

$$g(x) = \frac{1}{1 + b^2} \approx c$$

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{rocswappedproteindata.png}}
\caption{We re-run the algorithms from Figure 1 on the same dataset, but swapping the unlabeled positive examples for the labeled positive examples. Our new dataset contains 4558 negative examples, 2453 unlabeled positive examples, and only 348 labeled positive examples.  Ceiling LR still performs significantly better than baselines and previous work, although not as well as knowing the full dataset.  These results are impressive because less than 5\% of the examples are labeled, all of which are positive labels. The weighted SVM and other SVMs may not be optimally calibrated.}
\label{rocswapped}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{simulated.png}}
\caption{Simulating two gaussians as above, we can compare logistic regression on the true labels as compared to logistic regression, and POLR on a positive-only dataset with various values of $c$.}
\label{synthetic}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Positive-Only Logistic Regression}

Positive-Only Logistic Regression (POLR) is based on multinomial logistic regression, a multi-class generalization of binary logistic regression also known as softmax regression.  In standard 3-class multinomial logistic regression,

$$p(k=1 | x, w_1, w_2, w_3) =  \frac{e^{w_1 \cdot x}}{Z_N}$$
$$p(k=2 | x, w_1, w_2, w_3) =  \frac{e^{w_2 \cdot x}}{Z_N}$$
$$p(k=3 | x, w_1, w_2, w_3) =  \frac{e^{w_3 \cdot x}}{Z_N}$$

$Z_N$ is the normalization factor, the sum of the 3 numerators, and $\sum_{k=1,2,3}{p(k_j=k|x_j, w_1, w_2, w_3)} = 1$.

POLR models the positive-only dataset directly with a 3-class multinomial regression with the following classes: one class is positive and labeled ($P_L$), one is positive and unlabeled ($P_U$), and the last is negative and unlabeled (N).  There are no negative labeled samples.  We use the standard multinomial regression probabilities, but we share the weights $w_p$ between the positive-labeled ($P_L$) and positive-unlabeled ($P_U$) classes. Finally, we include a bias variable $b$ and a constant $q$ which bias the constant fraction of the positive samples which are labeled to be between $minimumC$ and 1.  We calculate $minimumC$ empirically as the fraction of the training set which are labeled examples.  $Z_N$ is the normalization factor, the sum of the 3 numerators.

$$p(k=P_L | x, w_p, w_n) =  \frac{e^{\mathbf{w_p} \cdot x - \mathbf{b}} + \mathbf{q \cdot e^{w_p \cdot x}}}{Z_N}$$
$$p(k=P_U | x, w_p, w_n) =  \frac{e^{\mathbf{w_p} \cdot x}}{Z_N}$$
$$p(k=N | x, w_p, w_n) =  \frac{e^{w_n \cdot x}}{Z_N}$$

This simplifies (proof in the Supplementary Materials) to

$$ p(k=P_L | x, w) = \frac{e^{-b} + q}{1 + e^{-b} + e^{w \cdot x}}$$
$$ p(k=P_U | x, w) =  \frac{1}{1 + e^{-b} + e^{w \cdot x}}$$
$$ p(k=N | x, w) = \frac{e^{w \cdot x}}{1 + e^{-b} + e^{w \cdot x}}$$

Where $w = w_n - w_p$. Notice that, $\forall x$,

$$\frac{p(k=P_L | x, w)}{p(k=P_L | x, w) + p(k=P_U | x, w)} = \frac{1}{1 + q + e^{-b}} = c$$

Thus, $c$ is the constant fraction of positive samples which are labeled. The term $q$ ensures that $c$ stays between $minimumC$ and $1$, instead of  between $0$ and $1$.

$$q = \frac{1}{1 - minimumC} - 1$$

\subsection{Learning $\beta = b, w$ from $D$}

Assuming we have independently drawn data $D$, the conditional log likelihood, where $\beta = b, w$ is

\begin{eqnarray*}
\log{p(D | \beta)} &=& \log \prod_{j=1...n}{p(\ell_j, x_j | \beta)} \\
 &=& \sum_{j=1...n}{\log{p(\ell_j, x_j | \beta)}} \\
 &=& \sum_{j=1...n}{\log{p(\ell_j | x_j, \beta)}} \cdot p(x_j | \beta)
\end{eqnarray*}

where $\ell_j$ is an indicator variable describing whether the sample is labeled or not. $\ell_j = 1$ if and only if $k_j=P_L$, otherwise $k_j = N$ or $k_j=P_U$ and $\ell_j = 0$.  Also note that $p(x_j|\beta) = p(x_j)$ and we assume an uninformative uniform prior for $x_j$, so let $p(x_j|\beta) = 1$.

The maximum likelihood (ML) estimate $\hat \beta$ is the value of $\beta$ maximizing the likelihood of the data $D$:
$$
\hat \beta = arg max_{\beta} p(D | \beta) = arg max_{\beta} \log{p(D | \beta)}.
$$

We calculate the gradients (derivations in the Supplementary Materials) in order to perform stochastic gradient ascent to maximize the log likelihood.

\begin{eqnarray*}
\nabla_{w_i}{Err_R (x_j, \beta)} = x_{j,i} \cdot
    \big(	I(\ell_j=0) \cdot p(k_j=N | \ell_j=0) \\
		- p(k_j=N)
    \big)
\end{eqnarray*}

\begin{figure}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{changingodds.png}}
\caption{The $w$ parameter updates change the odds of the positive and negatives overall, but does not change $c$ the probability of being labeled. Updates on this parameter change the likelihood of the observed labels while also changing the overall probability of positive labels in the same direction. More positive or negative data will push it in the appropriate direction. $n_j = p(k=N|x_j,w)$ and $p_j = 1-n_j$. The likelihood is maximized when $c \cdot p_{j}$ is maximal for labeled points and minimized for unlabeled points. }
\label{changingodds}
\end{center}
\vskip -0.2in
\end{figure}

\begin{eqnarray*}
\nabla_{b}{Err_R (x_j, \beta)}  = \Big( \frac{q}{Z} - p(k=P_L | x_j,\beta) \Big) \cdot \\
	\left( 
		\frac{I(\ell_j=1)}{p(k=P_L | x_j,\beta)} + 1 
	  \right)
\end{eqnarray*}

\begin{figure}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{changingc.png}}
\caption{The $b$ parameter updates change $c$ the probability of being labeled, but does not affect the proportion of positive to negative at each example. Updates to $b$ increase the likelihood of the observed data only without affecting the proportion of positive vs negative data overall. It is a weighted average of the observed label proportions.  It reaches a stable point when $\sum_{j}{p(\ell_j=1|x_j,w)}$ is the true proportion of labeled data.}
\label{changingc}
\end{center}
\vskip -0.2in
\end{figure}

We do not regularize the weights in our implementation.

\section{Experiments}

\subsection{Synthetic Data}

Figure \ref{synthetic} reproduce a version of similar gaussians generated in \cite{elkan08}. 

\subsection{SwissProt Dataset}

The algorithms were run on the SwissProt \cite{elkan08} dataset of 7359 proteins which each have a sparse set of 24081 binary features related to characteristics of the proteins. It contains 4558 negative examples, 2453 labeled positive examples, and 348 unlabeled positive examples.   We consider the negative and unlabeled positive examples together as the unlabeled dataset.

\section{Related Work}

The positive-only case can be seen as a special case of label-noise with 0 probability of labeling negative data. Bootkrajang and Kaban $\yrcite{bootkrajang12}$ designed a robust logistic regression algorithm based on binary logistic regression that has an extra variable indicating the true labels.  Unlike POLR, robust logistic regression is not convex and requires multiple restarts with different initializations and does not guarantee an optimal result. Tibshirani and Manning $\yrcite{tibshirani13}$ created a robust logistic regression algorithm with  shift parameters in the exponential term.  It is a convex model but it adds $N$ parameters to the model where $N$ are the number of samples.  This makes the model much more complicated and is problematic if $N >> P$.

\section{Future Work}

Ideally, we would like to be able to relax the assumption that $p(\ell_j|x)=c$ for all $x$.  In addition, it may be possible to extend Positive-Only Logistic Regression to the semi-supervised setting, where we also have some negative labels.  It would also be interesting to extend this to the multi-class or multi-label setting, especially since it is already based on multinomial logistic regression.  Finally, the true label ends up being a hidden variable in the model. It would be interesting to see if we can model more complex hidden variable interactions with logistic regression.

\section{Conclusions}

\bibliography{icml2014}
\bibliographystyle{icml2014}

\onecolumn

\section{Appendix A: Derivations of Stochastic Updates for Positive-Only Logistic Regression}

\subsection{Positive-Only Logistic Regression Model}

Let $q = \frac{1}{1 - minimumC} - 1$.

$$
p(k=P_L | x, w_p, w_n) =  \frac{e^{w_p \cdot x - b} + e^{w_p \cdot x + \log{q}}}{Z_{POLR}} 
				     = \frac{e^{w_p \cdot x - b} + q \cdot e^{w_p \cdot x}}{(1 + e^{-b} + q) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} 
$$
$$
p(k=P_U | x, w_p, w_n) =  \frac{e^{w_p \cdot x}}{Z_{POLR}} 
				     = \frac{e^{w_p \cdot x}}{(1 + e^{-b} + q) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} 
$$
$$
p(k=N | x, w_p, w_n) =  \frac{e^{w_n \cdot x}}{Z_{POLR}} 
				 = \frac{e^{w_n \cdot x}}{(1 + e^{-b} + q) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} 
$$
And then dividing every term by $w_p$ and setting $w = w_n - w_p$.
$$
p(k=P_L | x, w_p, w_n) = \frac{e^{w_p \cdot x - b} + q \cdot e^{w_p \cdot x}}{(1 + e^{-b} + q) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} 
 				     = \frac{e^{-b} + q}{1 + q + e^{-b} + e^{w \cdot x}}
$$
$$
p(k=P_U | x, w_p, w_n) = \frac{e^{w_p \cdot x}}{(1 + e^{-b} + q) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} 
				     = \frac{1}{1 + q + e^{-b} + e^{w \cdot x}}
$$
$$
p(k=N | x, w_p, w_n) = \frac{e^{w_n \cdot x}}{(1 + e^{-b} + q) \cdot e^{w_p \cdot x} + e^{w_n \cdot x}} 
				= \frac{e^{w \cdot x}}{1 + q + e^{-b} + e^{w \cdot x}}
$$


So we only need to learn the vector $w$.

\subsection{Gradients}

We want to use stochastic gradient following to learn the parameters $\beta$. So take the partial derivatives of the log conditional likelihood (LCL) for each parameter.

\subsubsection{$ \nabla_{w_i}{Err_R (D, \beta)}$}

$$
\nabla_{w_i}{Err_R (D, \beta)} = \frac{\partial}{\partial w_i} \log{p(D|\beta)} =  \sum_{j=1...n}{\frac{\partial}{\partial w_i}\log{p(\ell_j | x_j, \beta)}}
$$

so for each $i,j$, ($Z = 1 + q + e^{-b} + e^{w \cdot x}$),


\begin{eqnarray*}
\nabla_{w_{i,j}}{Err_R (x_j, \beta)} &=& \frac{\partial}{\partial w_i}\log{p(\ell_j | x_j, \beta)} \\
 &=& \frac{\partial}{\partial w_i}
		\log{\Bigg(
			I(\ell_j=1) \cdot \left( \frac{e^{-b} + q}{Z} \right) + 
			I(\ell_j=0) \cdot \left(\frac{1}{Z} + \frac{e^{w \cdot x}}{Z} \right)
		\Bigg)} \\
 &=& \frac{\partial}{\partial w_i}
		\log{\Bigg(
		    \frac{
			I(\ell_j=1) \cdot \left( e^{-b} + q \right) + 
			I(\ell_j=0) \cdot \left(1 + e^{w \cdot x} \right)
		     }{Z}
		\Bigg)} \\
 &=& \frac{\partial}{\partial w_i}
		\log{\bigg(
			I(\ell_j=1) \cdot \left( e^{-b} + q \right) +
			I(\ell_j=0) \cdot \left( 1 + e^{w \cdot x} \right)
		\bigg)}
		- \frac{\partial}{\partial w_i}
		 \log{Z} \\
 &=& I(\ell_j=1) \cdot \frac{\partial}{\partial w_i}
		\log{(e^{-b} + q)} +
    I(\ell_j=0) \cdot \frac{\partial}{\partial w_i}
		\log{ \left( 1 + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial w_i}
		 \log{Z} \\
 &=& I(\ell_j=0) \cdot \frac{\partial}{\partial w_i}
		\log{  \left( 1 + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial w_i}
		 \log{Z} \\
 &=& I(\ell_j=0) \cdot 
 		\frac{
			e^{w \cdot x}
		}{
			1 + e^{w \cdot x}
		} \cdot x_{j,i}
		- \frac{\partial}{\partial w_i}
		 \log{Z} \\
 &=& I(\ell_j=0) \cdot 
 		\frac{
			e^{w \cdot x}
		}{
			1 + e^{w \cdot x}
		} \cdot x_{j,i}
		- \frac{e^{w \cdot x}}{
			1 + e^{-b} + e^{w \cdot x}
		} \cdot x_{j,i} \\
 &=& x_{j,i} \cdot
    \big(	I(\ell_j=0) \cdot p(k_j=N | \ell_j=0, x_j, \beta) - 
		p(k_j=N | x_j, \beta)
    \big) \\
\end{eqnarray*}

\subsubsection{$ \nabla_{b}{Err_R (D, \beta)}$}

Similarly for the parameter $b$ in $\beta$, for each $j$,

\begin{eqnarray*}
\nabla_{b}{Err_R (x_j, \beta)} &=& \frac{\partial}{\partial c}\log{p(\ell_j | x_j, \beta)} \\
&=& I(\ell_j=1) \cdot \frac{\partial}{\partial b}
		\log{(e^{-b} + q)} +
    I(\ell_j=0) \cdot \frac{\partial}{\partial b}
		\log{ \left( 1 + e^{w \cdot x} \right) }
		- \frac{\partial}{\partial b}
		 \log{Z} \\
&=& I(\ell_j=1) \cdot \frac{\partial}{\partial b}
		\log{(e^{-b} + q)} -
		\frac{\partial}{\partial b}
		 \log{Z} \\
 &=& I(\ell_j=1) \cdot \frac{\partial}{\partial b}
		\log{  \left( e^{-b} + q \right) }
		- \frac{\partial}{\partial b}
		 \log{\bigg(
			1 + q + e^{-b} + e^{w \cdot x}
		\bigg)} \\
 &=& - I(\ell_j=1) \cdot
		\frac{e^{-b}}{e^{-b} + q}
		+
		 \frac{e^{-b}}{
			1 + q + e^{-b} + e^{w \cdot x}
		} \\
 &=& - I(\ell_j=1) \cdot
		\frac{ \left( p(k=P_L | x_j,\beta) - \frac{q}{Z} \right) }{p(k=P_L | x_j,\beta)}
		+
		 p(k=P_L | x_j,\beta) - \frac{q}{Z} \\
 &=& \Big( p(k=P_L | x_j,\beta) - \frac{q}{Z} \Big) \cdot \left( 
		1 - \frac{ I(\ell_j=1)}{p(k=P_L | x_j,\beta)}
	  \right) 
\end{eqnarray*}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
